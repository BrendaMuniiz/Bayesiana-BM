---
title: "Bayesiana P1"
author:
  - Brenda da Silva Muniz 11811603
  - Mônica Amaral Novelli 11810453
date: "Setembro 2021"
output: pdf_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```
```{r pkg-load, message = FALSE}
library(tidyverse)
library(effectsize)
library(invgamma)
```

# Problema dado

Considera-se um conjunto histórico de dados de densidade do solo em uma região registrados por Henry Cavendish no século XVIII. Agora, supõe-se que, de experimentos e medições prévios, a priori para θ, densidade da terra, é considerada ser N(5,4; 0,01). O pesquisador registrou 23 medidas da densidade do solo. Para estes dados, temos a média de y = 5,48 e supõe-se aqui que a variância de seu erro de medida é conhecida e igual a 0,04.

## Priori informativa

Conforme nos foi dado no enunciado, temos que a distribuição Priori informativa é dada por  N(5,4; 0,01) sendo $\theta \sim \mathcal{N}(y_0, \tau_0^2)$ com média $y_0$ e variância $\tau_0^2$

## #gráfico da distribuição priori informativa
mu.0 <- 5.4
dp.0 <- 0.1
mu <- seq(4,7,0.01)
plot(mu,dnorm(mu,mu.0,dp.0),type="l",
     col="#FF0080",lty=1,pch=18,   
     xlab=expression(mu),
     ylab=expression(pi(mu)))
legend(4, 3.7,
       legend=c("Priori informativa"),
       col=c("#FF0080"),
       lty=1:2,pch=c(7,6),cex=0.6)


## Priori não informativa de Jeffreys

Temos que distribuição não informativa de Jeffreys é dada por:

$p(θ) ∝ [I(θ)]^{1/2}$

Em que $I(\theta)$ é a medida de informação esperada de Fisher de θ através de X que é definida como:

$I(θ) = E[-\frac{∂^2log p(x|\theta)}{∂\theta^2}]$


## Verossimilhança

Dados $X_1, X_2, . . . , X_n$ amostra aleatória $X ∼ N(μ, σ^2)$

Temos que a função de densidade $f_X(x)$ é dada por:

$f_X(x) = \frac{1}{√2πσ^2}\exp({-\frac{1}{2σ^2}(x − μ)})$

E nossa função de distribuição conjunta $f_{X_1,X_2,...,X_n}(x_1, x_2, . . . , x_n)$


$\prod_{i = 1}^{n} f_X(x_i) =  \prod_{i=1}^{n} \frac{1}{\sqrt{2\piσ^2}}\exp(-\frac{1}{{2σ^2}}(x_i − μ)^2 = (\frac{1}{\sqrt{2\piσ^2}})^{n/2}\exp(-\frac{1}{{2σ^2}}\sum_{i=1}^{n} (x_i − μ)^2)$

Temos que:

$\sum_{i=1}^{n}(x_i − μ)^2 =  \sum_{i=1}^{n} (x_i -\overline{x} )^2 + 2(\overline{x} - μ) \sum_{i=1}^{n}(x_i -\overline{x} ) + \sum_{i=1}^{n}(\overline{x} - μ)^2  $

E, desprezando as constantes, ficamos com:

$ \sum_{i=1}^{n} (x_i − μ)^2   ∝ n(μ - \overline{x})^2 $

Desse modo, para uma amostra de tamanho n, a função de verossimilhança pode ser escrita como:

$ L(θ) = L(μ) ∝ \exp(-\frac{1}{2σ^2}\sum_{i=1}^{n}(x_i − μ)^2 ∝ \exp(−\frac{n}{2σ^2}(μ − \overline{x})^2) $


```{r}
# informacao dos dados: dist. normal com variancia conhecida = sigma2
 n = 23
 xbarra = 5.48
 var(dados)
 var(dados)/n
 sigma2 = 0.04 # variancia conhecida
```

```{r}
 # funcao de verossimilhanca
 f.vero = function(mu, dados) prod(dnorm(dados,mu,sqrt(sigma2)))
 mu.seq = seq(0,10,0.01)
 vero = sapply(mu.seq,f.vero,dados = dados)

```

```{r}
# grafico da funcao de verossimilhanca       
 plot(mu.seq,vero,  col = "blue", type= "l", xlab = expression(mu), ylab = "density")
```
```{r}
 # grafico da densidade da media dos dados
 plot(mu.seq,dnorm(mean(dados),mu.seq,sqrt(sigma2/n)), type="l",col = "green")

# informacao a priori
 nu = 10
 tau2 = 1
 lines(density(rnorm(1000,nu,sqrt(tau2))))

  # informacao a posteriori
 (media_posteriori = (n * xbarra * tau2 + nu * sigma2) / (n * tau2 + sigma2))
 (var_posteriori = sigma2 * tau2 / (n * tau2 + sigma2))
 lines(density(rnorm(1000,media_posteriori,sqrt(var_posteriori))), col = "red")
```

```{r}

```


## Posteriori

A distribuição a posteriori de θ dado x é $N(µ_1, {τ_1}^2)$ sendo

$µ1 = {\frac{{τ_0}^{−2}µ_0 + nσ^{−2}\overline{x}}{{τ_0}^{−2}+ nσ^{−2}}}$ e ${τ_1}^{−2} = {τ_0}^{-2} + nσ^{−2}.$




## Resumo a posteriori com a priori informativa

Para uma única observação vimos a família de distribuições normais é conjugada ao modelo normal. Para uma amostra de tamanho n, a função de verossimilhança pode ser escrita como

$ L(θ) = L(μ) ∝ \exp (−\frac{n}{2σ^2} (\overline{x} - \theta)^2}) $

onde os termos que não dependem de θ foram incorporados á constante de proporcionalidade. Portanto, a verossimilhança tem a mesma forma daquela baseada em uma única observação bastando substituir x por ${\overline{X}}$ e $σ^2$ por $σ^2/n$
Logo,  com as devidas substituições, i.e. a distribuição a posteriori de {/θ} dado x é
onde:
 = $\displaystyle \delta(X)=\frac{\tau_0^{-2}\mu_0+nσ^{-2}{\overline{X}}}{\tau_0^{-2}+nσ^{-2}}$

 e $\tau_1^{-2}=\tau_0^{-2}+nσ^{-2}$

Seguindo esta lógica,
Se $ X_1,\dots,X_n$ é uma amostra aleatória da $ N(\theta,σ^2)$ com $ σ^2$ conhecido e usarmos a priori conjugada, i.e. $ \theta\sim N(\mu_0,\tau_0^2)$ então a posteriori também será normal e neste caso média, mediana e moda coincidem. Portanto, o estimador de Bayes de $ \theta$ é dado por:
$$\displaystyle \delta(X)=\frac{\tau_0^{-2}\mu_0+nσ^{-2}{\overline{X}}}{\tau_0^{-2}+nσ^{-2}}$$

```{r}
estimador_de_bayes = ((1/0.01)*5.4+23*((1/(0.04*2))*5.48))/((1/0.01)+23*(1/(0.04*2)))
```

```{r tabdis5}
tibble(      Priori = c("Priori"),
             Media.pri = c(5.4),
             Media.pos = c(5.46),
             Mediana.post = c(5.46),
             Moda.post=c(5.46),
             SD.pri = c(0.1),
             SD.pos = c(0.2),
             IC.025 = c(qnorm(0.025, mean = 5.46 , sd = 0.2)),
             IC.975 = c(qnorm(0.975, mean = 5.46 , sd = 0.2)))%>%
  knitr::kable(digits = 2           , caption = "Resumo aposteriori (n=23)")
```

## Gráfico Posteriori com a priori informativa
Logo, de acordo com as fórmulas acima apresentadas temos que:

```{r}
mu.post <- 5.46
dp.post <- 0.00303

plot(mu,dnorm(mu,mu.post,dp.post),type="l",
     col="#00FFFF",lty=1,pch=18,   
     xlab=expression(mu),
     ylab=expression(pi(mu/y)))
```

## Resumo a posteriori com a priori não informativa

Temos que a média e a precisão da posteriori convergem para a média e a precisão amostrais. Com isso, a média, moda e mediana a posteriori coincidem então com a estimativa clássica de máxima verossimilhança, $ \overline{x}$.

O intervalo de confiança Bayesiano $100(1- \alpha) $% é dado por:

$(\overline{x} - z_{\alpha/2} \sigma/\sqrt{n} ; \overline{x} + z_{\alpha/2} \sigma/\sqrt{n})$

Com isso, temos nosso resumo a posteriori de modo que:

```{r tabdis5}
tibble(      Priori = c("Priori de Jeffreys"),
             Media.pos = c(5.48),
             Mediana.post = c(5.48),
             Moda.post=c(5.48),
             SD.pri = c(0.1),
             SD.pos = n/sigma2^2,
             IC.025 = c(qnorm(0.025, mean = 5.48 , sd = 0.2)),
             IC.975 = c(qnorm(0.975, mean = 5.48 , sd = n/sigma2^2)))%>%
  knitr::kable(digits = 2           , caption = "Resumo aposteriori (n=23)")
```


## Preditiva
