---
title: "Bayesiana P1"
author:
  - Brenda da Silva Muniz 11811603
  - Mônica Amaral Novelli 11810453
date: "Setembro 2021"
output: pdf_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# Problema dado

Considera-se um conjunto histórico de dados de densidade do solo em uma região registrados por Henry Cavendish no século XVIII. Agora, supõe-se que, de experimentos e medições prévios, a priori para θ, densidade da terra, é considerada ser N(5,4; 0,01). O pesquisador registrou 23 medidas da densidade do solo. Para estes dados, temos a média de y = 5,48 e supõe-se aqui que a variância de seu erro de medida é conhecida e igual a 0,04. Então, temos que a posteriori é simplesmente obtida através da fórmula acima como sendo θ|y ∼ N(5,46; 0,00303).

## Priori informativa

Conforme nos foi dado no enunciado, temos que a distribuição Priori informativa é dada por  N(5,4; 0,01) sendo $\theta \sim \mathcal{N}(y_0, \tau_0^2)$ com média $y_0$ e variância $\tau_0^2$

## #gráfico da distribuição priori informativa- (ver melhor mu) ?acrescentar aqui ou só depois das três distribuições juntas
mu.0 <- 5.4
dp.0 <- 0.1
mu <- seq(4,7,0.01)
plot(mu,dnorm(mu,mu.0,dp.0),type="l",
     col="#FF0080",lty=1,pch=18,   
     xlab=expression(mu),
     ylab=expression(pi(mu)))
legend(4, 3.7,
       legend=c("Priori informativa"),
       col=c("#FF0080"),
       lty=1:2,pch=c(7,6),cex=0.6)


## Priori não informativa de Jeffreys

Temos que distribuição não informativa de Jeffreys é dada por:

$p(θ) ∝ [I(θ)]^{1/2}$

Em que $I(\theta)$ é a medida de informação esperada de Fisher de θ através de X que é definida como:

$I(θ) = E[-\frac{∂^2log p(x|\theta)}{∂\theta^2}]$


## Verossimilhança

Dados $X_1, X_2, . . . , X_n$ amostra aleatória $X ∼ N(μ, σ^2)$

Temos que a função de densidade $f_X(x)$ é dada por:

$f_X(x) = \frac{1}{√2πσ^2}\exp({-\frac{1}{2σ^2}(x − μ)})$

E nossa função de distribuição conjunta $f_{X_1,X_2,...,X_n}(x_1, x_2, . . . , x_n)$

$\prod_{i = 1}^{n} f_X(x_i) =  \prod_{i=1}^{n} \frac{1}{\sqrt{2\piσ^2}}\exp(-\frac{1}{{2σ^2}}(x_i − μ)^2$


Para uma amostra de tamanho n, a função de verossimilhança pode ser escrita como:

$l(θ; x) = (2πσ^2)^{−n/2}\exp{\frac{−1}{2σ^2} \sum_{i=1}^{n} (x_i − θ)^2}∝ \exp{\frac{−n}{2σ^2} (\overline{x}− θ)^2}$


## Posteriori

A distribuição a posteriori de θ dado x é $N(µ_1, {τ_1}^2)$ sendo

$µ1 = {\frac{{τ_0}^{−2}µ_0 + nσ^{−2}\overline{x}}{{τ_0}^{−2}+ nσ^{−2}}}$ e ${τ_1}^{−2} = {τ_0}^{-2} + nσ^{−2}.$

## Gráfico Posteriori com a priori informativa
mu.post <- 5.46
dp.post <- 0.00303

plot(mu,dnorm(mu,mu.post,dp.post),type="l",
     col="#00FFFF",lty=1,pch=18,   
     xlab=expression(mu),
     ylab=expression(pi(mu/y)))

## Resumo a posteriori com a priori informativa

```{r tabdis5}
tibble(      Priori = c("Priori"),
             Media.pri = c(5.4),
             Media.pos = c(5.48),
             SD.pri = c(0.1),
             SD.pos = c(0.2),
             IC.025 = c(qnorm(0.025, mean = 5.48 , sd = 0.2)),
             IC.975 = c(qnorm(0.975, mean = 5.48 , sd = 0.2)))%>%
  knitr::kable(digits = 2, caption = "Resumo aposteriori (n=23)")
```

## Resumo a posteriori com a priori não informativa

Preencher aqui


```{r cars}
summary(cars) # teste
```
## Preditiva
