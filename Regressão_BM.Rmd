Este trabalho tem como objetivo ajustar um modelo de regressão linear múltipla a um conjunto de dados.

```{r setup, include=FALSE, warning=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(tidyverse)
library(knitr)
library(ggpubr)
library(gridExtra)
library(GGally)
library(pander)
```
## Conjunto de dados

O dataset contém dados de um experimento para determinar **tempo**, **temperatura**, **percentual de solvatação**, **rendimento de óleo** e **carvão total** sob o **rendimento (y)**

Significância: 97%


```{r}
library(readr)
dados <- read_csv("bla/data-table-B5.csv")
View(dados)
```

```{r}
#Renomeando as colunas
names(dados) <- c('y', 'Tempo', 'Temperatura', 'Perc_solvatação', 'Rendimento_Òleo', 'Carvão_Total','x6','x7')
head(dados)
```

Para facilitar nosso trabalho em termos computacionais, estaremos nomeando nossas variáveis e criando um data frame com as mesmas; sendo:

- $Y:$ Rendimento total de azeite por lote de amendoim*

- $X_1:$ Tempo

- $X_2:$ Temperatura

- $X_3:$ Perc_solvatação

- $X_4:$ Rendimento_Òleo

- $X_5:$ Carvão_Total

```{r}
x1 <- dados$Tempo
x2 <- dados$Temperatura
x3 <- dados$Perc_solvatação
x4 <- dados$Rendimento_Òleo
x5 <- dados$Carvão_Total
y <- dados$y

tabela01 <- data.frame(cbind(x1, x2, x3, x4, x5, y))
tabela01
```

Devido à complexidade das fórmulas envolvidas para realizarmos uma regressão linear múltima, utilizaremos uma abordagem matricial. Desse modo, poderemos escrever

$y_i = \beta_0 + \beta_1 x_{1i} + \beta_2 x_{2i} + ... + \beta_k x_{ki} + \varepsilon_i, i = 1, ..., 5. $

como:

$y1 = \beta_0 + \beta_1 x_{11} + \beta_2 x_{21} + ... + \beta_k x_{k1} + \varepsilon_1$
$y2 = \beta_0 + \beta_1 x_{12} + \beta_2 x_{22} + ... + \beta_k x_{k2} + \varepsilon_2$
$y3 = \beta_0 + \beta_1 x_{13} + \beta_2 x_{23} + ... + \beta_k x_{k3} + \varepsilon_3$
$y4 = \beta_0 + \beta_1 x_{14} + \beta_2 x_{24} + ... + \beta_k x_{k4} + \varepsilon_4$
$y5 = \beta_0 + \beta_1 x_{15} + \beta_2 x_{25} + ... + \beta_k x_{k5} + \varepsilon_5$

Assim, alocamos essas equações em dois vetores colunas (5x1), fazendo:
```{r}
n <- length(dados$y)
X <- matrix(c(rep(1,n), x1, x2, x3, x4, x5), ncol = 6, nrow = n, byrow = FALSE)
Y <- matrix(y, ncol = 1, nrow = n)
k <- ncol(X) -1
p <- k + 1
```

#vamos fazer análise descritiva ?

```{r}
#Definindo os betas do modelo de regressão múltipla
betas <- solve(t(X)%*%X)%*%t(X)%*%Y
betas

#Definindo a matrix C_jj
C_jj = solve(t(X)%*%X)

#Definindo uma matrix para os betas
betas <- matrix(data = betas, nrow = length(betas), ncol = 1, byrow = FALSE)
rownames(betas) <- c("beta0", "beta1","beta2","beta3","beta4","beta5")
betas
```

#conferir se é assim msm ou com todos os dados pois o resultado da diferente data = dados
```{r}
#Modelo do R
lm(formula = y ~., data = tabela01)
```
Segue abaixo a equação para o modelo determinado:

$y= 30.59320 - 0.38612x1 - 0.04556x2 + 0.31074x3 - 0.37168x4 + 0.01410x5$

#Teste de significancia da Regressão

### 3. Estimação de $\sigma^2$

$SQ_{res}=\boldsymbol{Y}^{T}\boldsymbol{Y}-\widehat{\boldsymbol{\beta}}^{T}\boldsymbol{X}^{T}\boldsymbol{Y}$

```{r}
SQRes <- (t(Y)%*%Y)-(t(betas)%*%t(X)%*%Y)
SQRes
```
#[,1]
#[1,] 3222.907

Logo, $\widehat{\sigma}^2=\dfrac{SQres}{n-p}$

```{r}
p <- ncol(X)

#estimando o sigma^2
sigma2 <- SQRes/(n-p)
sigma2
```
#[,1]
#[1,] 153.4718


## 4. ANOVA

Agora que ajustamos um modelo inicial, é necessário que verifiquemos se ele é adequado em explicar a variabilidade de nossa amostra. Vamos assumir que $\xi\sim N_n(0,\sigma^2I)$. Precisamos agora verificar nossa suposição graficamente:

```{r}

# Obtendo uma estimativa para Y a partir do modelo ajustado
Y_est <- X%*%betas

# Cálculo dos resíduos
res <- Y - Y_est
```

```{r}
p <- ggplot(tibble(res), aes(sample = res)) + stat_qq() + stat_qq_line() +
  labs(x = "Amostra",
       y = "Quantis Teóricos",
       title = "Normal Q-Q Plot") +
  theme_pubclean()

q <- ggplot(tibble(res), aes(res)) +
  geom_histogram(aes(y=..density..), binwidth = 4, stat = "bin") +
  labs(title = "Histograma dos resíduos",
       y = "Densidade",
       x = "Valor") +
  theme_pubclean()

grid.arrange(p,q, ncol = 2)

```

Interpretação: 

```{r}
pander(shapiro.test(res), style='rmarkdown',
       caption = "Teste de normalidade Shapiro-Wilk para os resíduos")
```
#| Test statistic | P value |
#|:--------------:|:-------:|
#|     0.9544     | 0.2733  |

#Table: Teste de normalidade Shapiro-Wilk para os resíduos

O teste acima confirma nossa suposição de que os resíduos têm distribuição Normal, pois, para um nível de significância de 97%, o valor-p obtido, 0,2733, não rejeita a hipótese nula, de normalidade dos dados.

Além disso, para determinar matematicamente se existe uma relação linear entre a variável resposta $\boldsymbol{Y}$ e qualquer as outras covariáveis $\boldsymbol{X}_1,\ldots,\boldsymbol{X}_k$, é possível utilizar o teste **ANOVA**. Nele, queremos testar:

**$H_0$**: Nenhuma das variáveis contribui significativamente ao modelo, versus:

**$H_a$**: Pelo menos uma das covariáveis contribui significativamente ao modelo.


Tabela *ANOVA*:

| **F.V.** | **G.L** | **S.Q.** | **Q.M.** | **F** |
|:---:|:---:|:---:|:---:|:---:|
| **Regressão** | $k$ | $SQ_{reg}$ | $QMreg=\dfrac{SQreg}{k}$ | $F=\dfrac{QMreg}{QMres}$ |
| **Resíduo** | $n-p$ | $SQ_{res}$ | $QMres=\dfrac{SQres}{n-p}$ |  |
| **Total** | $n-1$ | $SQ_{total}$ |  $QM_{Total}$  |  |
Table: Tabela ANOVA


```{r}
# Soma dos quadrados dos residuos  
(SQRes <- t(Y-Y_est)%*%(Y-Y_est))
```
# [,1]
# [1,] 3222.907

$$SQ_{Reg} = SQ_{Total} - SQ_{Res} = \frac 1n \sum_{i=1}^n y_i^2 - (Y-\hat{Y})^T\cdot(Y-\hat{Y}) =$$
$$\beta^T \cdot X^T \cdot Y - \frac1n (u^T \cdot Y)^2$$

```{r}
# Soma dos quadrados totais
u <- c(rep(1,n))
(SQTotal <- t(Y)%*%Y - ((t(u)%*%Y)^2)/n)
```
#[,1]
#[1,] 7870.112
```{r}
#Soma dos quadrados da regressao  
(SQReg <- SQTotal - SQRes)
```
#[,1]
#[1,] 4647.205

```{r}
#Calculando a anova
k <- 2 # covariaveis utilizadas
p <- k+1


gl_sqreg <- k
QMReg <- SQReg/gl_sqreg

gl_sqres <- n-p
QMRes <- SQRes/gl_sqres

gl_sqtotal <- n-1
```


```{r}
#calculando a estatistica F
(F_0 <- QMReg/QMRes)
```
#[,1]
#[1,] 17.30315
```{r}
(QMTotal <- QMRes + QMReg)
```
#[,1]
#[1,] 2457.89
Apresentamos os resultados obtidos na forma de tabela.

#achei o formato legal, copiei a tabela do código da tarefa 2, mudar valores
| **F.V.** | **G.L** | **S.Q.** | **Q.M.** | **F** |
|:---:|:---:|:---:|:---:|:---:|
| **Regressão** | $2$ | $9712.5$ | $4856.25$ | $97.05035$ |     
| **Resíduo** | $13$ | $650.5$ | $50.03846$ |  |
| **Total** | $15$ | $10363$ |  $4906.288$  |  |
Table: Tabela ANOVA para o modelo ajustado.


Como nosso estimador $F \sim F(k,\ n - k - 1)$, podemos obter os quantis com o auxílio do R:

```{r}
alpha <- 0.03
(RR <- qf(alpha, df1 = k, df2 = n - k -1, lower.tail = F) )
```
#[1] 4.072662

Rejeitamos $H_0$ se $F_0 > F_{crit}$, sendo $F_{crit}$ o quantil teorico da distribuicao F com $k$ e $n-p$ graus de liberdade.

```{r}

if(RR < F_0){
  cat("Rejeita-se H0")
}
```
#Rejeita-se H0

Dessa forma, podemos concluir com 97% de confiança que pelo menos uma das variáveis contribui significativamente ao modelo.







