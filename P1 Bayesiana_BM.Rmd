---
title: "Prova 1 de Inferência Bayesiana"
author:
  - Brenda da Silva Muniz 11811603
  - Mônica Amaral Novelli 11810453
date: "Outubro 2021"
output: pdf_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```
```{r pkg-load, message = FALSE}
library(tidyverse)
```

# Problema dado

Considera-se um conjunto histórico de dados de densidade do solo em uma região registrados por Henry Cavendish no século XVIII. Agora, supõe-se que, de experimentos e medições prévios, a priori para θ, densidade da terra, é considerada ser N(5,4; 0,01). O pesquisador registrou 23 medidas da densidade do solo. Para estes dados, temos a média de y = 5,48 e supõe-se aqui que a variância de seu erro de medida é conhecida e igual a 0,04.

```{r}
# pacote de dados utilizado:

# caso não esteja instalado, tirar do comentário
# install.packages("mas3321", repos="http://R-Forge.R-project.org")

library(mas3321)

data(cavendish)

dados<- cavendish[['earth_density']]

```

## _Priori_ informativa

Conforme nos foi dado no enunciado, temos que a distribuição Priori informativa é dada por  N(5,4; 0,01) sendo $\theta \sim \mathcal{N}(y_0, \tau_0^2)$ com média $y_0$ e variância $\tau_0^2$

### Gráfico da distribuição _priori_ informativa

```{r}
mu.0 <- 5.4
dp.0 <- 0.1
mu <- seq(4,7,0.01)
plot(mu,dnorm(mu,mu.0,dp.0),type="l",
     col="#FF0080",lty=1,pch=18,   
     xlab=expression(mu),
     ylab=expression(pi(mu)))
legend(4, 3.7,
       legend=c("Priori informativa"),
       col=c("#FF0080"),
       lty=1:2,pch=c(7,6),cex=0.6)
```

## _Priori_ não informativa de Jeffreys

Temos que distribuição a _priori_ não informativa de Jeffreys é dada por:

$p(θ) ∝ [I(θ)]^{1/2}$

Em que $I(\theta)$ é a medida de informação esperada de Fisher de θ através de X que é definida como:

$I(θ) = E[-\frac{∂^2log p(x|\theta)}{∂\theta^2}]$


## Verossimilhança

Dados $X_1, X_2, . . . , X_n$ amostra aleatória $X ∼ N(μ, σ^2)$

Temos que a função de densidade $f_X(x)$ é dada por:

$f_X(x) = \frac{1}{√2πσ^2}\exp({-\frac{1}{2σ^2}(x − μ)})$

E nossa função de distribuição conjunta $f_{X_1,X_2,...,X_n}(x_1, x_2, . . . , x_n)$

$\prod_{i = 1}^{n} f_X(x_i) =  \prod_{i=1}^{n} \frac{1}{\sqrt{2\piσ^2}}\exp(-\frac{1}{{2σ^2}}(x_i − μ)^2 = (\frac{1}{\sqrt{2\piσ^2}})^{n/2}\exp(-\frac{1}{{2σ^2}}\sum_{i=1}^{n} (x_i − μ)^2)$

Temos que:

$\sum_{i=1}^{n}(x_i − μ)^2 =  \sum_{i=1}^{n} (x_i -\overline{x} )^2 + 2(\overline{x} - μ) \sum_{i=1}^{n}(x_i -\overline{x} ) + \sum_{i=1}^{n}(\overline{x} - μ)^2  $

E, desprezando as constantes, ficamos com:

$ \sum_{i=1}^{n} (x_i − μ)^2   ∝ n(μ - \overline{x})^2 $

Desse modo, para uma amostra de tamanho n, a função de verossimilhança pode ser escrita como:

$ L(θ) = L(μ) ∝ \exp(-\frac{1}{2σ^2}\sum_{i=1}^{n}(x_i − μ)^2 ∝ \exp(−\frac{n}{2σ^2}(μ − \overline{x})^2) $


```{r}
# informacao dos dados: dist. normal com variancia conhecida = sigma2
 n = 23
 xbarra = 5.48
 var(dados) # testando a variância
 var(dados)/n
 sigma2 = 0.04 # variancia conhecida
```

```{r}
 # funcao de verossimilhança

 f.vero = function(mu, dados) prod(dnorm(dados,mu,sqrt(sigma2)))
 mu.seq = seq(0,10,0.01)
 vero = sapply(mu.seq,f.vero,dados = dados)

```

```{r}
# grafico da funcao de verossimilhanca       
 plot(mu.seq,vero,  col = "blue", type= "l", xlab = expression(mu), ylab = "density")
```

```{r}
 # grafico da densidade da media dos dados
 plot(mu.seq,dnorm(mean(dados),mu.seq,sqrt(sigma2/n)), type="l",col = "green")

# informacao a priori
 nu = 10
 tau2 = 1
 lines(density(rnorm(1000,nu,sqrt(tau2))))

  # informacao a posteriori
 (media_posteriori = (n * xbarra * tau2 + nu * sigma2) / (n * tau2 + sigma2))
 (var_posteriori = sigma2 * tau2 / (n * tau2 + sigma2))
 lines(density(rnorm(1000,media_posteriori,sqrt(var_posteriori))), col = "red")
```

```{r}

```


## Distribuição a _Posteriori_

A distribuição a _posteriori_ de θ dado x é $N(µ_1, {τ_1}^2)$ sendo:

$µ1 = {\frac{{τ_0}^{−2}µ_0 + nσ^{−2}\overline{x}}{{τ_0}^{−2}+ nσ^{−2}}}$ e ${τ_1}^{−2} = {τ_0}^{-2} + nσ^{−2}.$



## Resumo a _posteriori_ com a _priori_ informativa

Para uma única observação vimos a família de distribuições normais é conjugada ao modelo normal. Para uma amostra de tamanho n, a função de verossimilhança pode ser escrita como:

$ L(θ) = L(μ) ∝ \exp (−\frac{n}{2σ^2} (\overline{x} - \theta)^2}) $

Onde os termos que não dependem de θ foram incorporados á constante de proporcionalidade. Portanto, a verossimilhança tem a mesma forma daquela baseada em uma única observação bastando substituir x por ${\overline{X}}$ e $σ^2$ por $σ^2/n$
Logo,  com as devidas substituições, i.e. a distribuição a _posteriori_ de {/θ} dado x é:
 
 $μ_0=\frac{\tau_0^{-2}\mu_0+nσ^{-2}{\overline{X}}}{\tau_0^{-2}+nσ^{-2}}$

 e $\tau_1^{-2}=\tau_0^{-2}+nσ^{-2}$

Seguindo esta lógica,

Se $ X_1,\dots,X_n$ é uma amostra aleatória da $ N(\theta,σ^2)$ com $ σ^2$ conhecido e usarmos a _priori_ conjugada, i.e. $ \theta\sim N(\mu_0,\tau_0^2)$ então a _posteriori_ também será normal e neste caso média, mediana e moda coincidem. Portanto, o estimador de Bayes de $ \theta$ é dado por:
$$\displaystyle \delta(X)=\frac{\tau_0^{-2}\mu_0+nσ^{-2}{\overline{X}}}{\tau_0^{-2}+nσ^{-2}}$$

```{r}
estimador_de_bayes = ((1/0.01)*5.4+23*((1/(0.04*2))*5.48))/((1/0.01)+23*(1/(0.04*2)))
```

```{r tabdis5}
tibble(      Priori = c("Priori"),
             Media.pri = c(5.4),
             Media.pos = c(5.46),
             Mediana.post = c(5.46),
             Moda.post=c(5.46),
             SD.pri = c(0.1),
             SD.pos = c(0.2),
             IC.025 = c(qnorm(0.025, mean = 5.46 , sd = 0.2)),
             IC.975 = c(qnorm(0.975, mean = 5.46 , sd = 0.2)))%>%
  knitr::kable(digits = 2           , caption = "Resumo aposteriori (n=23)")
```

## Gráfico da distribuição a _Posteriori_ com a _priori_ informativa

Logo, de acordo com as fórmulas acima apresentadas temos que:

```{r}
mu.post <- 5.46
dp.post <- 0.00303

plot(mu,dnorm(mu,mu.post,dp.post),type="l",
     col="#00FFFF",lty=1,pch=18,   
     xlab=expression(mu),
     ylab=expression(pi(mu/y)))
```

## Resumo a _posteriori_ com a _priori_ não informativa

Temos que a média e a precisão da _posteriori_ convergem para a média e a precisão amostrais. Com isso, a média, moda e mediana a posteriori coincidem então com a estimativa clássica de máxima verossimilhança, $ \overline{x}$.

O intervalo de confiança Bayesiano $100(1- \alpha) $% é dado por:

$(\overline{x} - z_{\alpha/2} \sigma/\sqrt{n} ; \overline{x} + z_{\alpha/2} \sigma/\sqrt{n})$

Com isso, temos nosso resumo a posteriori de modo que:

```{r tabdis2}
tibble(      Priori = c("Priori de Jeffreys"),
             Media.pos = c(5.48),
             Mediana.post = c(5.48),
             Moda.post=c(5.48),
             SD.pri = c(0.1),
             SD.pos = n*sigma2^2,
             IC.025 = c(qnorm(0.025, mean = 5.48 , sd = 0.2)),
             IC.975 = c(qnorm(0.975, mean = 5.48 , sd = n*sigma2^2)))%>%
  knitr::kable(digits = 2           , caption = "Resumo aposteriori (n=23)")
```


## Preditiva a _priori_

A distribuição preditiva a _priori_, sob independência condicional é dada pela integral da verossimilhança (de uma única observação) multiplicada pela _posteriori_, conforme a fórmula abaixo:

$f(y|D) = \int_{-∞}^{∞} fY(y)π(θ)dμ$

Neste caso,

$ Y~N(\mu,\sigma^2 ), f_y(y) =  \frac{1}{\sqrt2{\pi}\sigma^2}e^{-\frac{1}{2 \sigma^2}(y-\mu)^2}  $
$ \theta~N(\mu_0,\sigma_0^2 ), \pi(\theta) =  \frac{1}{2{\pi}\sigma_0^2}e^{-\frac{1}{2 \sigma_0^2}(y-\mu_0)^2}  $

$f(y|D = \int_{-∞}^{∞}fy(y)π(θ)dμ )$
= $ \frac{1}{\sqrt2{\pi}\sigma^2}e^{-\frac{1}{2 \sigma^2}(y-\mu)^2} \frac{1}{\sqrt2{\pi}\sigma_0^2}e^{-\frac{1}{2 \sigma_0^2}(y-\mu_0)^2}d\mu$

Lembrando-se que $\sigma^2$, e os hiperparâmetros $\mu_0$ e $ \sigma_0$ são constantes conhecidas, tem-se:
$f(y|D = \int_{-∞}^{∞} e^{-\frac{1}{2 \sigma^2}(y-\mu)^2}e^{-\frac{1}{2 \sigma_0^2}(\mu-\mu_0)^2}d\mu$

Fazendo A= $\frac{1}{\sigma^2}$, B= $\frac{1}{\sigma_0^2}$, a=y, b= $\mu_0$, tem-se:

$f(y|D = \int_{-∞}^{∞} e^{- \frac{1}{2}[A(\mu-a)^2+B(\mu-b)^2]d\mu} $

Lembrando-se que:

$A(z-a)^2 + B(z-b)^2 = (A+B)(z-c)^2 + \frac{AB}{A+B}(a-b)^2 $

em que: $c = \frac{1}{A+B}(Aa+Bb)$

Tem-se,

$f(y|D)∝\int_{-∞}^{∞}e^{-\frac{1}{2}[(A+B)(μ−c)^2+\frac{AB}{A+B}(a-b)^2]}d\mu$

Portanto,

$f(y|D)∝e^{-\frac{1}{2}\frac{1}{\frac{A+B}{AB}}(a-b)^2}\int_{-∞}^{∞}e^{-\frac{1}{2}\frac{1}{A+B}(μ−c)^2}d\mu$

Em que:

$e^{-\frac{1}{2}\frac{1}{A+B}(\mu−c)^2} =$ Kernel$N(c, A+B)$ e, desse modo:

$\int_{-∞}^{∞}e^{-\frac{1}{2}\frac{1}{A+B}(\mu−c)^2}dμ = \sqrt{2\pi(A+B)}$

Como $ A = \frac{1}{\sigma^2}$, $B = \frac{1}{\sigma_0^2}$, $a = y$, $b = \mu_0$:

$\frac{A+B}{AB} = \frac{\frac{1}{\sigma^2} + \frac{1}{\sigma_0^2}}{\frac{1}{\sigma^2}\frac{1}{\sigma_0^2}}  = \sigma_0^2+\sigma^2, f (y |D) ∝ e^{-\frac{1}{2}\frac{1}{\sigma_0^2+ \sigma^2}(y-\mu_0)^2}$

Comparando-se com o kernel da distribuição $N(\mu, \sigma^2)$ conclui-se que a distribuição preditiva a _priori_ é dada por:

$y |D ∼ N(\mu_0,\sigma_0^2 + \sigma^2)$

Logo, substituindo na fórmula, temos que:
$y|D∼N(5.4,0.01+0.04) = N(5.4,0.05)$
